{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this assignment we have used web scrapping and created a dataset which has 3 divisions of health, climate and business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Create your own dataset for text classification. It should contain at least 1000 words in total and at least two categories with at least 100 examples per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "gr3l3gO4y1Gg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('dataset_aml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "K4tSQG9gJOD7"
   },
   "outputs": [],
   "source": [
    "df.replace('health',0,inplace=True)\n",
    "df.replace('climate',1,inplace=True)\n",
    "df.replace('business',2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "aAuRMfuzDGSy"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1ugPZ7FaC9X"
   },
   "source": [
    "#### 2. Split the dataset into training (at least 160examples) and test (at least 40 examples) sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "60FHAGaxyinN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "OmXtVFq27aJq"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0fTrL-AAEJtf",
    "outputId": "586b4bde-3ad4-449c-b087-eeb93c27ee06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 3)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tmFOj23PEMpZ",
    "outputId": "9bd7a8fa-81bb-4516-a8f0-a81e2d42b53e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "WLlGH29eEwda"
   },
   "outputs": [],
   "source": [
    "train_data=train_data.iloc[:,1:]\n",
    "test_data=test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "WC8uAHr6FWWK"
   },
   "outputs": [],
   "source": [
    "train_data_tkn = Dataset.from_pandas(train_data)\n",
    "test_data_tkn = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "4yudj-51Ni5x"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_train = tokenizer(train_data_tkn[\"text\"], return_tensors=\"np\", padding=True)\n",
    "tokenized_test = tokenizer(test_data_tkn[\"text\"], return_tensors=\"np\", padding=True)\n",
    "\n",
    "tokenized_train = dict(tokenized_train)\n",
    "tokenized_test = dict(tokenized_test)\n",
    "\n",
    "labels_train = np.array(train_data_tkn[\"label\"])\n",
    "labels_test = np.array(test_data_tkn[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmMAe2ViaKEW"
   },
   "source": [
    "#### 3. Fine tune a pretrained language model capable of generating text (e.g., GPT) that you can take from the Hugging Face Transformers library with the dataset your created (this tutorial could be very helpful: https://huggingface.co/docs/transformers/training). Report the test accuracy. Discuss what could be done to improve accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7b2NLsfwOk_y",
    "outputId": "e19f11e1-1736-422c-892c-a108b130c2c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 [==============================] - 108s 14s/step - loss: 1.1026 - sparse_categorical_accuracy: 0.3194\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 81s 13s/step - loss: 1.1007 - sparse_categorical_accuracy: 0.3417\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 80s 13s/step - loss: 1.0976 - sparse_categorical_accuracy: 0.3583\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 83s 14s/step - loss: 1.0883 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 80s 13s/step - loss: 1.1023 - sparse_categorical_accuracy: 0.3139\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "## Model Definition\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"BSC-TeMU/roberta-base-bne\", from_pt=True, num_labels=3)\n",
    "\n",
    "## Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.metrics.SparseCategoricalAccuracy()\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=loss,\n",
    "              metrics=metric) \n",
    "\n",
    "## Fitting the data \n",
    "history = model.fit(tokenized_train, labels_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tq7wW5tdQleJ",
    "outputId": "f55fc17e-86fd-4aa1-943d-949d626653c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 9s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[ 0.00409388, -0.17499132,  0.10081658],\n",
       "       [ 0.00410101, -0.17500289,  0.10081441],\n",
       "       [ 0.0041034 , -0.17501122,  0.10081749],\n",
       "       [ 0.00409702, -0.17500857,  0.10081347],\n",
       "       [ 0.00409907, -0.17501134,  0.10081636],\n",
       "       [ 0.00409364, -0.17500266,  0.10081191],\n",
       "       [ 0.00409187, -0.17498949,  0.10081265],\n",
       "       [ 0.00409186, -0.1749943 ,  0.10081364],\n",
       "       [ 0.00409374, -0.1749948 ,  0.10080694],\n",
       "       [ 0.00409551, -0.17500436,  0.1008113 ],\n",
       "       [ 0.00408841, -0.17498237,  0.10081255],\n",
       "       [ 0.00410164, -0.17500797,  0.10081722],\n",
       "       [ 0.00409571, -0.17500158,  0.10081238],\n",
       "       [ 0.00409209, -0.17499475,  0.10081135],\n",
       "       [ 0.00409792, -0.17501625,  0.10081596],\n",
       "       [ 0.00409924, -0.17500757,  0.10081818],\n",
       "       [ 0.0041016 , -0.17501405,  0.10081898],\n",
       "       [ 0.00409707, -0.1750015 ,  0.10081585],\n",
       "       [ 0.00408811, -0.1749983 ,  0.10081268],\n",
       "       [ 0.00409639, -0.17499745,  0.1008136 ],\n",
       "       [ 0.00408483, -0.1749966 ,  0.10081474],\n",
       "       [ 0.00409704, -0.17500983,  0.10081617],\n",
       "       [ 0.004098  , -0.17501129,  0.1008141 ],\n",
       "       [ 0.00409317, -0.17500204,  0.10081825],\n",
       "       [ 0.00410083, -0.17500164,  0.10080978],\n",
       "       [ 0.0041065 , -0.17501074,  0.10081782],\n",
       "       [ 0.0041008 , -0.17501307,  0.1008125 ],\n",
       "       [ 0.00409199, -0.17500287,  0.10081429],\n",
       "       [ 0.00409867, -0.17500123,  0.10080939],\n",
       "       [ 0.00409603, -0.17500618,  0.10081231],\n",
       "       [ 0.00408885, -0.17499658,  0.10081024],\n",
       "       [ 0.00409413, -0.17500153,  0.10081504],\n",
       "       [ 0.00409444, -0.1749994 ,  0.10081182],\n",
       "       [ 0.00408418, -0.17498794,  0.10081301],\n",
       "       [ 0.00409306, -0.17499292,  0.10080552],\n",
       "       [ 0.00409267, -0.17499724,  0.10081049],\n",
       "       [ 0.00410149, -0.17499804,  0.10081559],\n",
       "       [ 0.00410162, -0.17501068,  0.10081352],\n",
       "       [ 0.00409866, -0.17500442,  0.10081702],\n",
       "       [ 0.00409723, -0.17501128,  0.10081407],\n",
       "       [ 0.00410524, -0.17501041,  0.10081688],\n",
       "       [ 0.00409515, -0.17500255,  0.1008142 ],\n",
       "       [ 0.00410112, -0.1750152 ,  0.1008142 ],\n",
       "       [ 0.00409828, -0.17500985,  0.10081031],\n",
       "       [ 0.00409226, -0.17499346,  0.10081154],\n",
       "       [ 0.00409528, -0.17500114,  0.10081876],\n",
       "       [ 0.00409767, -0.17500892,  0.10081226],\n",
       "       [ 0.00410157, -0.17500705,  0.10081566],\n",
       "       [ 0.00409333, -0.17499924,  0.10081194],\n",
       "       [ 0.00408761, -0.17499663,  0.10081635],\n",
       "       [ 0.00409553, -0.17500058,  0.10081649],\n",
       "       [ 0.00409631, -0.17500171,  0.10081761],\n",
       "       [ 0.00409803, -0.1750049 ,  0.10081839],\n",
       "       [ 0.00409761, -0.17500232,  0.10080935],\n",
       "       [ 0.00409776, -0.17500234,  0.10080997],\n",
       "       [ 0.00410133, -0.17501529,  0.10081609],\n",
       "       [ 0.00410531, -0.17501746,  0.10081422],\n",
       "       [ 0.00409685, -0.17499739,  0.10081303],\n",
       "       [ 0.00410107, -0.17500325,  0.1008146 ],\n",
       "       [ 0.00409132, -0.17499177,  0.1008117 ],\n",
       "       [ 0.00410586, -0.17501897,  0.10081851],\n",
       "       [ 0.00409523, -0.1749994 ,  0.1008136 ],\n",
       "       [ 0.00410069, -0.17501482,  0.10081803],\n",
       "       [ 0.00409828, -0.17500499,  0.10081299],\n",
       "       [ 0.00409112, -0.17499839,  0.10081465],\n",
       "       [ 0.00410564, -0.17500888,  0.10082098],\n",
       "       [ 0.00409616, -0.17500642,  0.10081465],\n",
       "       [ 0.00409576, -0.17500143,  0.10081477],\n",
       "       [ 0.00409382, -0.1750055 ,  0.10081259],\n",
       "       [ 0.00408912, -0.17498338,  0.10080768],\n",
       "       [ 0.00410032, -0.1750098 ,  0.10081182],\n",
       "       [ 0.00409947, -0.17500657,  0.10081311],\n",
       "       [ 0.00410306, -0.17500642,  0.10081603],\n",
       "       [ 0.00409705, -0.17499927,  0.10082094],\n",
       "       [ 0.00410048, -0.17500131,  0.10080989],\n",
       "       [ 0.00410897, -0.17501917,  0.10081167],\n",
       "       [ 0.00410059, -0.17500654,  0.10081754],\n",
       "       [ 0.00408848, -0.17499845,  0.100814  ],\n",
       "       [ 0.00410119, -0.17499787,  0.10081179],\n",
       "       [ 0.00409442, -0.1750017 ,  0.10081225],\n",
       "       [ 0.00409538, -0.17500782,  0.10081448],\n",
       "       [ 0.00409279, -0.17500295,  0.10081028],\n",
       "       [ 0.00410142, -0.17500652,  0.10081462],\n",
       "       [ 0.0040913 , -0.1749998 ,  0.10081293],\n",
       "       [ 0.00409009, -0.1749853 ,  0.10080735],\n",
       "       [ 0.00409406, -0.174997  ,  0.10081661],\n",
       "       [ 0.00409941, -0.17501508,  0.10082104],\n",
       "       [ 0.00409094, -0.17499104,  0.10081432],\n",
       "       [ 0.00409335, -0.1749971 ,  0.10081472],\n",
       "       [ 0.00409285, -0.17500007,  0.10081602]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions= model.predict(tokenized_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DKtUvpXKQtEJ",
    "outputId": "c02bd236-ecdc-467a-94a0-dfafff546ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 8s 2s/step - loss: 1.1214 - sparse_categorical_accuracy: 0.2667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.12144136428833, 0.2666666805744171]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tokenized_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMHEWaZua-Ix",
    "outputId": "fd153562-8c69-49eb-ec4a-e9ba82b8fe4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 [==============================] - 110s 13s/step - loss: 1.1634 - sparse_categorical_accuracy: 0.3500\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 81s 13s/step - loss: 1.1348 - sparse_categorical_accuracy: 0.3194\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 82s 13s/step - loss: 1.1135 - sparse_categorical_accuracy: 0.3083\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 80s 13s/step - loss: 1.1232 - sparse_categorical_accuracy: 0.2806\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 83s 14s/step - loss: 1.1228 - sparse_categorical_accuracy: 0.3472\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "## Model Definition\n",
    "model2 = TFAutoModelForSequenceClassification.from_pretrained(\"BSC-TeMU/roberta-base-bne\", from_pt=True, num_labels=3)\n",
    "\n",
    "## Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.metrics.SparseCategoricalAccuracy()\n",
    "model2.compile(optimizer=optimizer, \n",
    "              loss=loss,\n",
    "              metrics=metric) \n",
    "\n",
    "## Fitting the data \n",
    "history2 = model2.fit(tokenized_train, labels_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMhJNomNbMQF",
    "outputId": "3279703b-c5e2-4d22-a00d-3b554d341fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 8s 2s/step - loss: 1.1146 - sparse_categorical_accuracy: 0.3444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1145516633987427, 0.3444444537162781]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(tokenized_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEAHlxG2bopf"
   },
   "source": [
    "There are multiple strategies to enhance the accuracy of a pre-trained language model like BSC-TeMU/roberta-base-bne on a text classification dataset. One method involves fine-tuning the model on the specific task using the dataset, where adjustments to hyperparameters and optimizing the loss function are made. Another effective approach is to employ data augmentation techniques such as back-translation or synonym replacement to diversify the training data. Furthermore, utilizing an ensemble of models or training the model with a larger corpus of data can elevate its accuracy. Lastly, regularization techniques like dropout or early stopping aid in improving the model's generalization performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
